Answers to the text questions go here.

PartOne:

D) Answers: 

The Flesch-Kincaid score can be useful, but it doesn’t always reflect the true difficulty of a text, especially when comparing works from 
different time periods or writing styles.

1. When texts span different time periods and language changes. Take Sense and Sensibility (1811) and Blood Meridian (1985) their 
FK scores are pretty close (10.89 for Sense and Sensibility and 11.44 for Blood Meridian). However, the language is quite different.
Sense and Sensibility uses long, formal 19th-century sentences, which can be harder to follow today. On the other hand, Blood Meridian
has a more minimalist style. Despite these differences in language and time, the scores are similar, showing how the the formula doesn’t 
really capture how language changes over time or how different writing styles affect readability.

2. When complexity goes beyond sentence structure. The FK score focuses mainly on sentence length and syllable count, but it doesn’t take 
into account the overall complexity of the language or the writer's style. For example, Erewhon (1872) has a highest score of 14.68, while 
The Secret Garden (1911) has a lowest score of 4.65, even though both can be challenging in their own ways.

In conclusion, the FK score doesn’t account for how language evolves or how writing style can affect how hard a text is to read, 
so it’s not always a reliable measure.




PartTwo:

F) Answers

I have two different classification modelling approaches. The first one is a standard machine learning 
setup using TfidfVectorizer with no custom tokenizer, combined with Random Forest and SVM classifiers. 
The second is a pipeline model where I included a custom tokenizer, SMOTE for oversampling, and the same classifiers.

The reason I structured the modelling in two separate approaches was to see if I could get more balanced results, 
especially given the dataset’s class imbalance. I wanted to test whether cleaning and processing the text more carefully 
and balancing the training data  would help the models perform better overall.

In the standard ML approach, I used TfidfVectorizer with English stopwords removed and ngram_range set to (1,2) or (1,3).
 This gave me a macro F1 score of 0.5116 for SVM and 0.3462 for Random Forest. For the pipeline version, I used a custom tokenizer 
 that applied regex cleaning, removed punctuation and stopwords, and lemmatised tokens with spaCy. I also added SMOTE in the pipeline 
 to oversample minority classes. With this setup, SVM scored 0.4898, and Random Forest improved slightly to 0.3653.

The macro F1 score averages the F1 scores across all classes equally, which highlights the model’s performance on underrepresented classes.
Since Independent and Liberal Democrat speeches were very limited in the dataset, their F1 scores were often close to 0(0 both modelling RF),
which significantly pulled down the overall macro average even when Conservative and Labour had strong precision and recall.

While the custom tokenizer helped make the text features cleaner and more interpretable, it didn’t significantly improve classification performance. 
In this task, n-gram coverage and class frequency seemed to matter more than token level cleaning. 